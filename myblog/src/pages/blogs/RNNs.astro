---
import Layout from '../../layouts/Layout.astro';

const frontmatter = {
  title: "Recurrent Neural Networks (RNNs) - Intuition, Math, and BPTT",
  description: "A ground-up explanation of RNNs, why memory matters, and how Backpropagation Through Time actually works.",
  pubDate: new Date("2026-01-01"),
  author: "Gagan",
  tags: ["Deep Learning", "RNN", "Neural Networks", "Backpropagation"],
};

const formattedDate = frontmatter.pubDate.toLocaleDateString('en-US', {
  year: 'numeric',
  month: 'long',
  day: 'numeric'
});

// LaTeX expressions stored as variables to avoid template literal conflicts
const latex = {
  // Inline expressions
  t: String.raw`$t$`,
  xt: String.raw`$x_t$`,
  ht_minus_1: String.raw`$h_{t-1}$`,
  ht: String.raw`$h_t$`,
  yt: String.raw`$y_t$`,
  Whh: String.raw`$W_{hh}$`,
  Wxh: String.raw`$W_{xh}$`,
  Why: String.raw`$W_{hy}$`,
  T: String.raw`$T$`,
  
  // Block expressions
  hiddenStateUpdate: String.raw`$$h_t = \tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$`,
  output: String.raw`$$y_t = W_{hy} h_t + b_y$$`,
  coreGradient: String.raw`$$\frac{\partial L}{\partial h_t} = \frac{\partial L_t}{\partial h_t} + \frac{\partial L}{\partial h_{t+1}} W_{hh}^T \odot (1 - \tanh^2(z_{t+1}))$$`,
  localError: String.raw`$$\frac{\partial L_t}{\partial h_t}$$`,
  futureError: String.raw`$$W_{hh}^T \odot (1 - \tanh^2(z_{t+1}))$$`,
  tanhDerivative: String.raw`$$\frac{d}{dz} \tanh(z) = 1 - \tanh^2(z)$$`,
  vanishingGradient: String.raw`$$(0.5)^T$$`,
};
---

<Layout title={`${frontmatter.title} | Gagan P`}>
  <!-- KaTeX Stylesheet for LaTeX rendering -->
  <link
    slot="head"
    rel="stylesheet"
    href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"
    integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV"
    crossorigin="anonymous"
  />

  <main class="container mx-auto px-4 py-8 sm:py-12">
    <div class="max-w-3xl mx-auto">
      <article class="prose prose-stone dark:prose-invert max-w-none">
        <!-- Header Section -->
        <div class="flex flex-col sm:flex-row sm:justify-between sm:items-start mb-4">
          <h1 class="font-zt text-4xl sm:text-5xl text-stone-900 dark:text-stone-100 mb-0">
            {frontmatter.title}
          </h1>

          <a
            href="/blogs"
            class="hidden sm:inline-flex font-space text-stone-600 dark:text-stone-400 hover:text-stone-900 dark:hover:text-stone-100 transition-colors items-center gap-2 mt-2 no-underline"
          >
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
              <path fill-rule="evenodd" d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z" clip-rule="evenodd" />
            </svg>
            /blogs
          </a>
        </div>

        <p class="font-space text-stone-500 dark:text-stone-400 text-sm mb-2">
          {formattedDate}
        </p>

        <div class="flex flex-wrap gap-2 mb-8">
          {frontmatter.tags.map((tag) => (
            <span class="font-space text-xs bg-stone-100 dark:bg-stone-800 text-stone-600 dark:text-stone-400 px-3 py-1 rounded-full">
              {tag}
            </span>
          ))}
        </div>

        <!-- Main Content -->
        <div class="font-space text-stone-700 dark:text-stone-300 leading-relaxed space-y-6">
          <p class="text-lg text-stone-600 dark:text-stone-400">
            So this is going to be a <strong>comprehensive blog on Recurrent Neural Networks (RNNs)</strong> - so far we have learnt about FCNNs, CNNs, etc. But they all have something missing, something which the human brain specialises in. Can you guess it?
          </p>
          <p class="text-lg text-stone-600 dark:text-stone-400">
            Ok let me spill it, it's memory!
          </p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            Why RNNs?
          </h2>

          <p>
            So before we move onto the concept of RNNs, let's discuss why it came into discussion in the first place. Regular neural networks, be it <strong>FCNNs, CNNs</strong>, etc. all work the same way:
          </p>

          <blockquote class="border-l-4 border-stone-300 dark:border-stone-600 pl-4 italic my-6">
            input → hidden layers → output
          </blockquote>

          <p>
            Once the output is produced, <strong>the network forgets everything</strong>.
          </p>

          <p>
            The next input you pass in the next time step has <strong>no relation</strong> to the previous one. The model is <strong>stateless</strong>.
          </p>

          <p>
            Think of it like HTTP: each request is independent, no memory of past requests.
          </p>

          <p>
            To be concise: <strong>vanilla neural networks do not have memory</strong>. It cannot refer to what the past input was and it cannot relate the present input with the past inputs, this limits the Neural Network from identifying trends, patterns, etc. And if you just look back, RNNs are one of the most fundamental concepts which lead to the current day technology in AI by unlocking the capability of NN's to have memory. 
          </p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            So Why Do We Need Memory?
          </h2>

          <p>Let's take a classic NLP example:</p>

          <blockquote class="border-l-4 border-stone-300 dark:border-stone-600 pl-4 italic my-6">
            "My dog has blue eyes, it loves its new pet toy that I bought."
          </blockquote>

          <p>When the model encounters the word <strong>"it"</strong>, what does it refer to?</p>

          <ul class="list-disc pl-6 space-y-2">
            <li>the dog?</li>
            <li>the eyes?</li>
            <li>the toy?</li>
          </ul>

          <p>
            A standard neural network has <strong>no idea</strong>, because it processes inputs <strong>independently</strong>.
          </p>

          <p>
            This is exactly the class of problems where <strong>context matters</strong>. Language, time series, speech, sensor data, sequences in general have a certain pattern/trend and we need memory to predict the next term in the sequence( The sequence could be language, audio, etc.)
          </p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            RNNs for the Rescue 
          </h2>

          <p>
            Instead of treating each input independently, an RNN processes inputs <strong>sequentially</strong>. Now what does this mean?
          </p>

          <p>
            An RNN is made up of <strong>recurrent cells</strong>. At every time step <span set:html={latex.t} />, the cell takes <strong>two inputs</strong>:
          </p>

          <ol class="list-decimal pl-6 space-y-2">
            <li>the current input <span set:html={latex.xt} /></li>
            <li>the previous hidden state <span set:html={latex.ht_minus_1} /></li>
          </ol>

          <p>It then produces:</p>
          <ul class="list-disc pl-6 space-y-2">
            <li>a new hidden state <span set:html={latex.ht} /></li>
            <li>optionally an output <span set:html={latex.yt} /></li>
          </ul>
          <p>Now this is where RNNs are different from regular NN's, instead of the output from one hidden layer passing through the next hidden layer, here in RNNs the hidden state produced in the first time step passes through the same Recurrent Cell in the next time step. Notice that the weights are shared here for all the time steps. We see something similar in CNNs where the kernel values are shared, and we know why sharing weights are such a huge improvement, both in learning the features and reducing the number of computations </p>

          <img src="/images/rnn.png" alt="RNN Architecture Diagram" class="w-full rounded-lg my-6" />
          <p class="text-xs text-stone-500 dark:text-stone-400 text-center -mt-4 mb-6">
            Image credit: <a href="https://authurwhywait.github.io/blog/2021/12/02/introduction_to_dl02/" target="_blank" rel="noopener noreferrer" class="underline hover:text-stone-700 dark:hover:text-stone-300">authurwhywait.github.io</a>
          </p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            Now What Is the Hidden State?
          </h2>

          <p>The hidden state is <strong>the memory of the network</strong>.</p>

          <p>Think of it as:</p>

          <blockquote class="border-l-4 border-stone-300 dark:border-stone-600 pl-4 italic my-6">
            "everything the network has understood so far compressed in a vector numerically"
          </blockquote>

          <p>Let's say you(RNN) are a student studying history, now here is how an RNN works: </p>
          <ul class="list-disc pl-6 space-y-2">
            <li>You initialise an empty Hidden state vector</li>
            <li>You take in the first embedding of the word from your textbook as the input at time step 0, now you encode this information into the hidden state</li>
            <li>Now you take in the next word at time step 1, now you use the knowledge you learnt from the previous word to process the input and then write the new encoded knowledge to the hidden state</li>
            <li>This goes on and on</li>
            <li>At any time step you can make a prediction y, from the current hidden state</li>
          </ul>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            The Core RNN Equations
          </h2>

          <p><strong>Hidden state update:</strong></p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.hiddenStateUpdate} />

          <p><strong>Output:</strong></p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.output} />

          <p>Where:</p>
          <ul class="list-disc pl-6 space-y-2">
            <li><strong><span set:html={latex.Whh} /></strong> → recurrent (memory) weights</li>
            <li><strong><span set:html={latex.Wxh} /></strong> → input weights</li>
            <li><strong><span set:html={latex.Why} /></strong> → output weights</li>
          </ul>

          <h3 class="font-zt text-2xl sm:text-3xl text-stone-900 dark:text-stone-100 mt-8 mb-3">
            A Key Property: Weight Sharing
          </h3>

          <p>The same weights are used at every time step.</p>

          <p>This is what allows RNNs to generalize across sequences of different lengths.</p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            Backpropagation Through Time (BPTT)
          </h2>

          <p>Here in RNNs we use some thing known as Backpropagation Through Time, now as the name suggests, we travel back through the time steps to reduce the loss. BPTT is just backpropagation with a twist.</p>

          <p>Instead of going backward through layers, we go backward through <strong>time</strong>.</p>

          <h3 class="font-zt text-2xl sm:text-3xl text-stone-900 dark:text-stone-100 mt-8 mb-3">
            The Big Picture
          </h3>

          <p>An RNN unrolled through time looks like a very deep network:</p>

          <pre class="bg-stone-900 dark:bg-stone-950 text-stone-100 p-4 rounded-lg overflow-x-auto"><code>x₁ → h₁ → h₂ → h₃ → ... → h_T</code></pre>

          <p>Each hidden state depends on the previous one. So when we compute gradients, the error must flow backward across time steps.</p>

          <h3 class="font-zt text-2xl sm:text-3xl text-stone-900 dark:text-stone-100 mt-8 mb-3">
            The Core Gradient Equation
          </h3>

          <p>The heart of BPTT is this( It's a wonderful derivation, hope you work it out on paper ):</p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.coreGradient} />

          <p>This single equation explains everything.</p>

          <h3 class="font-zt text-2xl sm:text-3xl text-stone-900 dark:text-stone-100 mt-8 mb-3">
            Intuition: Two Sources of Blame
          </h3>

          <p>The hidden state <span set:html={latex.ht} /> is blamed in two ways:</p>

          <p><strong>1. Local Error (Present)</strong></p>

          <p>If the prediction at time t is wrong:</p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.localError} />

          <p>This is the immediate mistake.</p>

          <p><strong>2. Future Error (Butterfly Effect)</strong></p>

          <p><span set:html={latex.ht} /> also influences all future hidden states. So if something goes wrong later, <span set:html={latex.ht} /> is partially responsible.</p>

          <p>That error flows back through:</p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.futureError} />

          <h3 class="font-zt text-2xl sm:text-3xl text-stone-900 dark:text-stone-100 mt-8 mb-3">
            The tanh "Valve"
          </h3>

          <p>The derivative of tanh is:</p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.tanhDerivative} />

          <p>This acts like a valve:</p>
          <ul class="list-disc pl-6 space-y-2">
            <li>if neuron is active → gradient flows</li>
            <li>if neuron is saturated → gradient dies</li>
          </ul>

          <img src="/images/rnn_bptt.png" alt="Backpropagation Through Time Visualization" class="w-full rounded-lg my-6" />
          <p class="text-xs text-stone-500 dark:text-stone-400 text-center -mt-4 mb-6">
            Image credit: <a href="https://authurwhywait.github.io/blog/2021/12/02/introduction_to_dl02/" target="_blank" rel="noopener noreferrer" class="underline hover:text-stone-700 dark:hover:text-stone-300">authurwhywait.github.io</a>
          </p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            Why Gradients Vanish in RNNs
          </h2>

          <p>Now let's address the main issue in RNNs, which is vanishing gradients. We all know that the range of tanh(z) is [-1, 1]. So the derivative as we saw aboove should always be <=1. So Each timestep multiplies the gradient by a value less than 1.</p>

          <p>After <span set:html={latex.T} /> steps( if we take 0.5 as the average value ):</p>

          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.vanishingGradient} />

          <p>For large <span set:html={latex.T} />, this becomes almost zero.</p>
          
          <div class="my-6 p-4 bg-stone-50 dark:bg-stone-800/50 rounded-lg overflow-x-auto" set:html={latex.coreGradient} />
          <p>If we just look back to the loss equation, after many timesteps, the gradient simply does not flow from the future because the tanh derivative part becomes very close to 0 and the weights stop updating, or in other words the network stops learning </p>

          <p>That's why: <strong>Vanilla RNNs only remember short-term dependencies</strong></p>

          <hr class="border-stone-200 dark:border-stone-700 my-8" />

          <h2 class="font-zt text-3xl sm:text-4xl text-stone-900 dark:text-stone-100 mt-10 mb-4">
            Let's discuss the Limitations of Vanilla RNNs
          </h2>

          <ul class="list-disc pl-6 space-y-2">
            <li>Vanishing gradients</li>
            <li>Poor long-term memory</li>
            <li>Training becomes unstable for long sequences</li>
            <li>Difficult to capture long-range dependencies in language</li>
          </ul>
          <p> All these things led to a newer architecture using the base concept of RNNs, called LSTMs which we will learn about in the next blog.</p>
          <p>I hope this blog gave you a baseline intuition on RNNs</p>
          <p>Signing off, Gagan</p>
        </div>

        <!-- Mobile Back Button -->
        <div class="mt-12 pt-8 border-t border-stone-200 dark:border-stone-700 sm:hidden">
          <a
            href="/blogs"
            class="font-space text-stone-600 dark:text-stone-400 hover:text-stone-900 dark:hover:text-stone-100 transition-colors inline-flex items-center gap-2 no-underline"
          >
            <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5" viewBox="0 0 20 20" fill="currentColor">
              <path fill-rule="evenodd" d="M9.707 16.707a1 1 0 01-1.414 0l-6-6a1 1 0 010-1.414l6-6a1 1 0 011.414 1.414L5.414 9H17a1 1 0 110 2H5.414l4.293 4.293a1 1 0 010 1.414z" clip-rule="evenodd" />
            </svg>
            /blogs
          </a>
        </div>
      </article>
    </div>
  </main>

  <!-- KaTeX auto-render for any $...$ or $$...$$ syntax -->
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"
    integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8"
    crossorigin="anonymous">
  </script>
  <script
    defer
    src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
    integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05"
    crossorigin="anonymous">
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function() {
      if (typeof renderMathInElement !== "undefined") {
        renderMathInElement(document.body, {
          delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "$", right: "$", display: false}
          ]
        });
      }
    });
  </script>
</Layout>

<style>
  article {
    animation: fadeIn 0.6s ease-out forwards;
  }

  @keyframes fadeIn {
    from {
      opacity: 0;
      transform: translateY(20px);
    }
    to {
      opacity: 1;
      transform: translateY(0);
    }
  }

  /* Ensure proper spacing for headings */
  article h2 {
    scroll-margin-top: 2rem;
  }

  article h3 {
    scroll-margin-top: 2rem;
  }

  /* Style for inline code */
  article code {
    font-weight: 500;
    font-family: 'Monaco', 'Consolas', monospace;
    font-size: 0.9em;
    background: rgba(0, 0, 0, 0.05);
    padding: 0.2em 0.4em;
    border-radius: 4px;
  }

  @media (prefers-color-scheme: dark) {
    article code {
      background: rgba(255, 255, 255, 0.1);
    }
  }

  /* Remove underline from navigation links */
  article a.no-underline {
    text-decoration: none;
  }

  /* KaTeX styling adjustments */
  :global(.katex) {
    font-size: 1.1em;
  }
  
  :global(.katex-display) {
    margin: 0;
    padding: 0;
  }
</style>